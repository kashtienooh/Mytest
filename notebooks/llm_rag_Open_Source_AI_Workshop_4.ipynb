{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKC_pi8xJU5e"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ovaccarelli/LLM-RAG/blob/main/notebooks/llm_rag_Open_Source_AI_Workshop_4.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "# üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBG-N771JU5g"
      },
      "outputs": [],
      "source": [
        "# Install all required Python packages for this workshop\n",
        "\n",
        "!pip install wget langchain_ollama langchain-community faiss-cpu pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuHZTfeyJU5h"
      },
      "outputs": [],
      "source": [
        "import os, wget\n",
        "from pathlib import Path\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1DhwxkbR0A"
      },
      "source": [
        "#### Install Ollama and download a pre-trained LLM model\n",
        "Open the terminal with `%xterm`\n",
        "\n",
        "- Download and run Ollama‚Äôs installer (only once per session): `curl https://ollama.ai/install.sh | sh`.\n",
        "- Start the Ollama API server in the background: `ollama serve &`\n",
        "\n",
        "- Download a pre-trained LLM (e.g., Mistral) for local inference: `ollama pull mistral`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CatEy7YhJU5j"
      },
      "outputs": [],
      "source": [
        "# Install the colab-xterm extension\n",
        "\n",
        "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-kJWXK4vYy8"
      },
      "outputs": [],
      "source": [
        "%xterm\n",
        " # curl https://ollama.ai/install.sh | sh\n",
        " # ollama serve &\n",
        " # ollama pull mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzxZE8YozcGW"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4hoFkvWJU5l"
      },
      "source": [
        "### üåê Download PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdc6c78iJU5m"
      },
      "outputs": [],
      "source": [
        "# Create the \"data/PDFs\" folder if it doesn't exist\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ovaccarelli/LLM-RAG/main/data/PDFs/Open_Source_AI_workshop.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (PDF_FOLDER / name).is_file():\n",
        "        filename = wget.download(url, f\"data/PDFs/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VJ0NOSPJU5m"
      },
      "source": [
        "### Construct the vectorstore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jj2ualTJU5n"
      },
      "outputs": [],
      "source": [
        "# 1. Create a folder to store the vector index\n",
        "VECTORSTORES_DIR = Path(\"data/vectorstores\")\n",
        "os.makedirs(VECTORSTORES_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Point to the directory containing our PDFs\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "\n",
        "# 3. Use PyPDFDirectoryLoader to load every PDF page as a Document\n",
        "loader = PyPDFDirectoryLoader(PDF_FOLDER)\n",
        "documents = loader.load()\n",
        "\n",
        "# 4. Verify how many pages are loaded\n",
        "print(f\"Loaded {len(documents)} PDF pages\")\n",
        "\n",
        "# Set chunk size (how many characters per chunk) and overlap\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 100\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Split the loaded PDFs into smaller, overlapping chunks\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# Define the embedding model ‚Äî BGE is a strong open-source embedding model for English\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
        "\n",
        "embedding_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={\"device\": \"cpu\"},  # \"cuda\" if you run locally with a GPU\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# Create a FAISS index from the text chunks and their embeddings\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
        "\n",
        "# Save the vectorstore locally for reuse\n",
        "vectorstore.save_local(VECTORSTORES_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze4WmNqKD-Zd"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd1zzDB4HT3e"
      },
      "source": [
        "## üß© Final Step: Connect Retriever + LLM to Answer Questions\n",
        "\n",
        "Now that we have a vectorstore built from our documents and a local LLM (like Mistral) running, we're ready to complete the RAG pipeline. This means combining document retrieval with LLM-based answering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k4H8WwrH0At"
      },
      "source": [
        "### üìù Define a Custom Prompt Template\n",
        "We create a custom prompt that instructs the model to:\n",
        "\n",
        "- Use only the retrieved document chunks as context.\n",
        "\n",
        "- Avoid hallucinating or inventing answers.\n",
        "\n",
        "- Respond concisely (max 3 sentences).\n",
        "\n",
        "- Stick to English for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2A3VmRsNO-y"
      },
      "outputs": [],
      "source": [
        "# Define the prompt template to guide the LLM behavior\n",
        "\n",
        "rag_prompt = \"\"\"\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{input}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Wrap the prompt string in a LangChain PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"input\"],\n",
        "    template=rag_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_lWw83eJeo7"
      },
      "source": [
        "### üîç Build the Retrieval-Augmented Generation (RAG) Chain\n",
        "\n",
        "Now that we have our vectorstore populated with document chunks and embeddings, we can wire everything together into a Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "This RAG system uses a \"stuffing\" strategy, where all retrieved documents are concatenated into a single prompt before being passed to the language model.\n",
        "\n",
        "#### üîó Key Components:\n",
        "\n",
        "- **`vectorstore.as_retriever`**  \n",
        "  Converts the vectorstore into a retriever object that finds the most semantically relevant document chunks for a user query.  \n",
        "\n",
        "- **`create_stuff_documents_chain(...)`**  \n",
        "  Creates a LangChain chain that stuffs multiple documents into a prompt template and sends it to the LLM.  \n",
        "  This strategy is effective when the total input size remains within the LLM‚Äôs context window.\n",
        "\n",
        "- **`create_retrieval_chain(...)`**  \n",
        "  Wraps the retriever and the document chain into a full end-to-end pipeline:\n",
        "  1. A user query is passed to the retriever.\n",
        "  2. The retriever returns a list of relevant text chunks.\n",
        "  3. These chunks are inserted into a prompt template along with the original question.\n",
        "  4. The LLM generates an answer strictly based on the provided context.\n",
        "\n",
        "This architecture gives you a fully functional, local, open-source LLM-based assistant that can answer domain-specific questions using real documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReXQDoGENSxY"
      },
      "outputs": [],
      "source": [
        "# Define the LLM\n",
        "\n",
        "llm = OllamaLLM(\n",
        "    model=\"...\",\n",
        "    temperature=...,\n",
        "    stop=[\"<end_of_turn>\"],\n",
        ")\n",
        "\n",
        "# Build the LLM question-answering chain\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        ")\n",
        "\n",
        "# Configure the retriever\n",
        "\n",
        "NB_RETRIEVED_CHUNKS = ...\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": NB_RETRIEVED_CHUNKS}\n",
        ")\n",
        "\n",
        "# Combine the retriever + LLM chain into one Retrieval-Augmented Generation (RAG) pipeline\n",
        "\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain=question_answer_chain\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhUOjmIoJU5q"
      },
      "source": [
        "The temperature parameter in a language model (LLM) controls the randomness of the model's output.\n",
        "\n",
        "- A lower temperature value (closer to 0) makes the model more deterministic, favoring higher probability words and resulting in more predictable and repetitive text.\n",
        "\n",
        "- A higher temperature value (closer to 1) increases randomness, allowing for more creative and diverse responses by giving less probable words a better chance of being chosen.\n",
        "\n",
        "Adjusting the temperature helps balance between coherence and creativity in the generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh4YVPuYKyWJ"
      },
      "source": [
        "### Chatting with our RAG chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drPeKORpNWm3"
      },
      "outputs": [],
      "source": [
        "query = \"...\"\n",
        "result = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "# ‚úÖ Print the generated answer\n",
        "console.print(Markdown(result[\"answer\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32u0WSbzmuUY"
      },
      "outputs": [],
      "source": [
        "# Retrieve the top relevant chunks\n",
        "retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "# Print the retrieved chunks\n",
        "print(f\"\\nüîç Top {NB_RETRIEVED_CHUNKS} Retrieved Chunks:\\n\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"--- Chunk #{i} ---\")\n",
        "    print(doc.page_content[:500].strip(), \"\\n\")  # print first 500 characters of each chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCy327uILLk4"
      },
      "source": [
        "Display the full result object (includes context docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSgCCh4yNhzk"
      },
      "outputs": [],
      "source": [
        "console.print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}