{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia01YLTJJBPq"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ovaccarelli/LLM-RAG/blob/main/notebooks/llm_rag_Open_Source_AI_Workshop_final.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "# üîß 0. Setup\n",
        "\n",
        "Before we start building our Retrieval-Augmented Generation (RAG) system, we need to install the necessary libraries.\n",
        "This will set up the core components for document loading, vector storage, LLM orchestration, and environment handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDQ5VGMSZD1r"
      },
      "source": [
        "### üì¶ Installation\n",
        "\n",
        "> ‚úÖ **Note:** This might take some minutes. Once completed, you're ready to start working with our LLM-Retrieval-Augmented Generation (RAG)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfLOr5Q8ZQfx"
      },
      "outputs": [],
      "source": [
        "# Install all required Python packages for this workshop\n",
        "\n",
        "!pip install langchain langchain-community faiss-cpu pymupdf pypdf sentence_transformers rich wget python-dotenv cryptography langchain_ollama langchain-docling pymupdf4llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddhkpq5QazeP"
      },
      "source": [
        "\n",
        "\n",
        "### üê≥ Installing and Running Ollama in Colab\n",
        "\n",
        "To run Ollama (a local LLM server) inside Google Colab, we‚Äôll open an interactive terminal session, download and install Ollama, then start the Ollama service and pull a model.\n",
        "\n",
        "If you prefer running Ollama on your PC, just follow the same procedure on your local shell (admin rights required), and consult the official docs at ollama.ai.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DntQzEbhbEH1"
      },
      "source": [
        "#### Enable a Terminal in Colab  \n",
        "We‚Äôll use the `colab-xterm` extension to spawn a bash shell directly in your notebook.\n",
        "\n",
        "The `colab-xterm` package creates a new browser-based terminal in Colab.\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KJQQP3ZvVI5"
      },
      "outputs": [],
      "source": [
        "# Install the colab-xterm extension\n",
        "\n",
        "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1DhwxkbR0A"
      },
      "source": [
        "#### Install Ollama and download a pre-trained LLM model\n",
        "Open the terminal with `%xterm`\n",
        "\n",
        "- Download and run Ollama‚Äôs installer (only once per session): `curl https://ollama.ai/install.sh | sh`.\n",
        "- Start the Ollama API server in the background: `ollama serve &`\n",
        "\n",
        "- Download a pre-trained LLM (e.g., Mistral) for local inference: `ollama pull mistral`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-kJWXK4vYy8"
      },
      "outputs": [],
      "source": [
        "%xterm\n",
        " # curl https://ollama.ai/install.sh | sh\n",
        " # ollama serve & ollama pull mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9o2ejCVqccJ"
      },
      "source": [
        "#### üß† Check Installed Models\n",
        "\n",
        "You can use the command below to see which LLMs are currently available in your local Ollama environment.  \n",
        "This includes all models you've already downloaded (e.g., `mistral`), along with their sizes and versions.\n",
        "\n",
        "You can also explore and experiment with many other open-source models available on Ollama by browsing their collection here:  \n",
        "üëâ [https://ollama.com/search](https://ollama.com/search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzxZE8YozcGW"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIGIZzjHnIrF"
      },
      "source": [
        "#### ‚úÖ Verify Your LLM Setup with LangChain\n",
        "\n",
        "Now that your Ollama server is running and the model is pulled, let‚Äôs check if everything is connected properly.\n",
        "\n",
        "We‚Äôll use `LangChain`'s `OllamaLLM` class to send a test prompt to the **Mistral** model running locally via Ollama.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrE6aOW05gqC"
      },
      "outputs": [],
      "source": [
        "# Verify your Ollama-backed LLM is working with LangChain\n",
        "\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "# Initialize the OllamaLLM wrapper with the 'mistral' model you pulled\n",
        "llm = OllamaLLM(model=\"mistral\")\n",
        "\n",
        "# Generate a simple test completion\n",
        "response = llm.generate([\"Hello, how are you today?\"])\n",
        "\n",
        "# Print out the first generated text\n",
        "print(response.generations[0][0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaIZdNUrAHqW"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC8USy4p6_Hs"
      },
      "source": [
        "# üìÑ 1. Start Your LLM-RAG Pipeline: Load PDFs and Ask Your First Question\n",
        "\n",
        "Now that we have our local LLM (e.g., `mistral`) running via Ollama, let's build the first part of our Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "In this section, you'll:\n",
        "\n",
        "- Download several PDFs\n",
        "- Use LangChain to ingest and process these documents\n",
        "- Set up your local LLM with a simple custom prompt\n",
        "- Run your first query using LangChain's `PromptTemplate` + `OllamaLLM` integration\n",
        "\n",
        "We'll start with a basic prompt-based LLM chain. Later, we'll add document embeddings, a retriever, and a full RAG chain.\n",
        "\n",
        "### üõ†Ô∏è Let's Get Started:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqYW2IE5pvK"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "from pathlib import Path\n",
        "\n",
        "import langchain\n",
        "import wget\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from langchain_docling import DoclingLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_docling.loader import ExportType\n",
        "import pymupdf4llm\n",
        "from langchain_core.documents.base import Document\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdncOHha7tHT"
      },
      "source": [
        "### üåê Download PDFs\n",
        "We download a small collection of PDFs. These will be our example knowledge base for the RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY7mjipcHoqr"
      },
      "outputs": [],
      "source": [
        "# Create the \"data/PDFs\" folder if it doesn't exist\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ovaccarelli/LLM-RAG/main/data/PDFs/Open_Source_AI_workshop.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (PDF_FOLDER / name).is_file():\n",
        "        filename = wget.download(url, f\"data/PDFs/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqJTo9dm78Ds"
      },
      "source": [
        "### ü§ñ Initialize the Local LLM via Ollama\n",
        "We load the `mistral` model using the `OllamaLLM` wrapper from LangChain.  \n",
        "We also define some decoding parameters, like temperature and stopping conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTwkOsJmIhRK"
      },
      "outputs": [],
      "source": [
        "# Load your local LLM\n",
        "\n",
        "llm = OllamaLLM(\n",
        "    model=\"mistral\",\n",
        "    temperature=0.1,  # Will be explained later\n",
        "    stop=[\"<end_of_turn>\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUW9iv8h8nsN"
      },
      "source": [
        "### üßæ Define a Custom Prompt Template\n",
        "We craft a basic prompt to structure how questions are sent to the LLM.\n",
        "This format is compatible with LangChain's `PromptTemplate` system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHsWGtTPLMHd"
      },
      "outputs": [],
      "source": [
        "# Create a simple prompt to ask a question\n",
        "\n",
        "template = \"\"\"\n",
        "You are an helpful assistant that answer the question in detail.\n",
        "\n",
        "Human input: {question}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ECUsDI85Ml"
      },
      "source": [
        "### üîó Create a Simple LangChain LLM Chain\n",
        "We create a basic chain that connects the prompt to the LLM.\n",
        "This is the foundation for more complex RAG workflows (which we'll build later).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw326outLSHW"
      },
      "outputs": [],
      "source": [
        "llm_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jylanr_J-FED"
      },
      "source": [
        "### Ask a Question!\n",
        "We test the chain by sending a question about the AI-Days event.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w42Qcq7zLTcz"
      },
      "outputs": [],
      "source": [
        "result = llm_chain.invoke(input=\"who wrote this article?\")\n",
        "\n",
        "console.print(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAT9i_6D-OO-"
      },
      "source": [
        "> ‚úÖ **Note:** The LLM returns a response based on its pretrained knowledge (not yet using the PDFs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPoWurPW_5gb"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gs5RIuCATIX"
      },
      "source": [
        "## 2. Extract Text from a Single PDF\n",
        "\n",
        "In this step, we‚Äôll load one PDF file and convert its pages into plain text (or Markdown) using three different methods:\n",
        "\n",
        "- **PyPDFLoader** (LangChain): A straightforward loader that splits the PDF into page-level `Document` objects.  \n",
        "- **PyMuPDF4LLM**: A fast, native extractor that generates Markdown-formatted text with optional page-wise chunking.  \n",
        "- **Docling**: A robust parser that preserves layout and exports content as Markdown, either per page (DOC_CHUNKS) or whole-document (MARKDOWN).\n",
        "\n",
        "You will see how to:\n",
        "\n",
        "1. Read the PDF from disk.  \n",
        "2. Extract every page‚Äôs text into a structured format.  \n",
        "3. Time each method to compare performance.  \n",
        "4. Preview a specific page for verification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsxUf8ZFCaiT"
      },
      "source": [
        "### üìÅ Setup Paths & Choose only 1 PDF for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V409t4BG0xw"
      },
      "outputs": [],
      "source": [
        "# Create the \"data/sample_pdf\" folder if it doesn't exist\n",
        "SAMPLE_PDF_DIR = Path(\"data/sample_pdf\")\n",
        "os.makedirs(SAMPLE_PDF_DIR, exist_ok=True)\n",
        "\n",
        "# URL of the PDFs to test\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ovaccarelli/LLM-RAG/main/data/sample_pdf/2312.10997.pdf\",\n",
        "    \"https://raw.githubusercontent.com/ovaccarelli/LLM-RAG/main/data/sample_pdf/2312.10997_page13.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (SAMPLE_PDF_DIR / name).is_file():\n",
        "        filename = wget.download(url, f\"data/sample_pdf/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyiWgsgf-7Mb"
      },
      "source": [
        "#### PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRr5M6QM-Ccx"
      },
      "outputs": [],
      "source": [
        "pdf_path = SAMPLE_PDF_DIR/\"2312.10997.pdf\"  # Just pick one page for testing\n",
        "\n",
        "# Load the PDF with PyPDFLoader\n",
        "start = time.time()\n",
        "loader = PyPDFLoader(str(pdf_path))\n",
        "docs_pypdf = loader.load()                 # returns a list of Document objects, one per page\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Using file: {pdf_path.name}\")\n",
        "print(f\"üïí PyPDFLoader loaded {len(docs_pypdf)} pages in {end - start:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r0rl1Qb7Icb"
      },
      "outputs": [],
      "source": [
        "# --- Preview the PDF contents ---\n",
        "# Pages are indexed starting from 0\n",
        "\n",
        "page_to_print = 12  # Change this to the page index you want\n",
        "max_num_characters = 1000 # Change the max num of characters you want to print\n",
        "\n",
        "# Now preview the chosen page:\n",
        "\n",
        "if 0 <= page_to_print < len(docs_pypdf):\n",
        "    content = docs_pypdf[page_to_print].page_content\n",
        "    print(f\"--- üìÑ Page {page_to_print + 1} / {len(docs_pypdf)} ---\\n\")\n",
        "    print(content[:max_num_characters])\n",
        "else:\n",
        "    print(f\"Page {page_to_print} is out of range (max:{len(docs_pypdf)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fg81Ftk4NqN"
      },
      "source": [
        "### PyMuPDF4LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGSv0GGPBteE"
      },
      "outputs": [],
      "source": [
        "# Load the PDF with PyMuPDF4LLM\n",
        "start = time.time()\n",
        "docs_pymupdf = pymupdf4llm.to_markdown(str(pdf_path), page_chunks=True)       # return a list of page dicts\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Using file: {pdf_path.name}\")\n",
        "print(f\"üïí PyMuPDF4LLM extracted {len(docs_pymupdf)} pages in {end - start:.2f} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzqls88v7QH0"
      },
      "outputs": [],
      "source": [
        "# --- Preview the PDF contents ---\n",
        "# Pages are indexed starting from 0\n",
        "\n",
        "page_to_print = 12  # Change this to the page index you want\n",
        "max_num_characters = 1000 # Change the max num of characters you want to print\n",
        "\n",
        "# Now preview the chosen page:\n",
        "\n",
        "if 0 <= page_to_print < len(docs_pymupdf):\n",
        "    md = docs_pymupdf[page_to_print][\"text\"]\n",
        "    print(f\"--- üìÑ Page {page_to_print + 1} / {len(docs_pymupdf)} ---\\n\")\n",
        "    print(md[:max_num_characters])\n",
        "else:\n",
        "    print(f\"Page {page_to_print} is out of range (max:{len(docs_pymupdf)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E-f6svc4TLP"
      },
      "source": [
        "### Docling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ79kroX_ygW"
      },
      "outputs": [],
      "source": [
        "pdf_path_docling = SAMPLE_PDF_DIR/\"2312.10997_page13.pdf\"  # Just pick one page for testing\n",
        "\n",
        "# Load the PDF with Docling\n",
        "start = time.time()\n",
        "loader_docling = DoclingLoader(str(pdf_path_docling), export_type=ExportType.MARKDOWN)\n",
        "docs_docling = loader_docling.load()\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Using file: {pdf_path_docling.name}\")\n",
        "print(f\"üïí Docling loaded {len(docs_docling)} document(s) in {end - start:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyZ74jEnBExT"
      },
      "outputs": [],
      "source": [
        "# --- Preview the PDF contents ---\n",
        "\n",
        "# Print the full extracted text\n",
        "for idx, doc in enumerate(docs_docling):\n",
        "    print(f\"\\n--- üìÑ PDF Document: {pdf_path_docling.name} ---\\n\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKgX3jOJ7OZT"
      },
      "source": [
        "## 3. Construct the vectorstore\n",
        "\n",
        "In this step, we take the PDF documents and transform them into a searchable vector database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0IUq-0eF1_x"
      },
      "outputs": [],
      "source": [
        "# 1. Create a folder to store the vector index\n",
        "VECTORSTORES_DIR = Path(\"data/vectorstores\")\n",
        "os.makedirs(VECTORSTORES_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Point to the directory containing our PDFs\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "\n",
        "# 3. Use PyPDFDirectoryLoader to load every PDF page as a Document\n",
        "loader = PyPDFDirectoryLoader(PDF_FOLDER)\n",
        "documents = loader.load()\n",
        "\n",
        "# 4. Verify how many pages are loaded\n",
        "print(f\"Loaded {len(documents)} PDF pages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69xtJTpC0uk"
      },
      "source": [
        "### ‚úÇÔ∏è Split Documents into Chunks\n",
        "\n",
        "We break documents into smaller overlapping chunks using `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "- `chunk_size`: The number of characters per chunk.\n",
        "\n",
        "- `chunk_overlap`: Ensures that we maintain context between chunks.\n",
        "\n",
        "This is crucial for preserving semantic meaning across sentences and paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u2AIMgmMNHw"
      },
      "outputs": [],
      "source": [
        "# Set chunk size (how many characters per chunk) and overlap\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 100\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Split the loaded PDFs into smaller, overlapping chunks\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(all_splits)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjxcY36DCc_"
      },
      "source": [
        "### üîç Convert Text Chunks to Embeddings\n",
        "\n",
        "We now convert each text chunk into a high-dimensional vector using the BGE model (`BAAI/bge-large-en-v1.5`). These vectors capture the semantic meaning of the text.\n",
        "\n",
        "- We use `HuggingFaceBgeEmbeddings from LangChain`.\n",
        "\n",
        "- Normalizing embeddings helps improve similarity search accuracy.\n",
        "\n",
        "- We set the device to \"cpu\" for compatibility with Colab. (If you're running this on a local machine with GPU, you can switch \"cpu\" to \"cuda\" for better performance.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0UZzf5GMQ0c"
      },
      "outputs": [],
      "source": [
        "# Define the embedding model ‚Äî BGE is a strong open-source embedding model for English\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
        "\n",
        "embedding_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={\"device\": \"cpu\"},  # \"cuda\" if you run locally with a GPU\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDIcoz_DWI4"
      },
      "source": [
        "### üèóÔ∏è Create and Save the Vectorstore\n",
        "\n",
        "Using the text chunks and embeddings, we build our vectorstore:\n",
        "\n",
        "- FAISS (Facebook AI Similarity Search) is a fast library for vector similarity search.\n",
        "\n",
        "- This index will let us retrieve the most relevant chunks given a user question.\n",
        "\n",
        "We also save the vectorstore locally so that it can be reused later without recomputing everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4XZVZMpNKmC"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS index from the text chunks and their embeddings\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
        "\n",
        "# Save the vectorstore locally for reuse\n",
        "vectorstore.save_local(VECTORSTORES_DIR)\n",
        "\n",
        "print(\"‚úÖ Vectorstore created and saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5XOWboD24O"
      },
      "source": [
        "üíæ Reload the Vectorstore (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whi6g_8TEEoJ"
      },
      "outputs": [],
      "source": [
        "# You can reload the saved vectorstore anytime without recomputing everything\n",
        "vectorstore = FAISS.load_local(\n",
        "    VECTORSTORES_DIR,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True  # Required in Colab environments\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vectorstore reloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze4WmNqKD-Zd"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd1zzDB4HT3e"
      },
      "source": [
        "## üß© Final Step: Connect Retriever + LLM to Answer Questions\n",
        "\n",
        "Now that we have a vectorstore built from our documents and a local LLM (like Mistral) running, we're ready to complete the RAG pipeline. This means combining document retrieval with LLM-based answering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k4H8WwrH0At"
      },
      "source": [
        "### üìù Define a Custom Prompt Template\n",
        "We create a custom prompt that instructs the model to:\n",
        "\n",
        "- Use only the retrieved document chunks as context.\n",
        "\n",
        "- Avoid hallucinating or inventing answers.\n",
        "\n",
        "- Respond concisely (max 3 sentences).\n",
        "\n",
        "- Stick to English for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2A3VmRsNO-y"
      },
      "outputs": [],
      "source": [
        "# Define the prompt template to guide the LLM behavior\n",
        "\n",
        "rag_prompt = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Don't try to make up an answer and only use the information you know.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "You must answer in English.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{input}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Wrap the prompt string in a LangChain PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"input\"],\n",
        "    template=rag_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_lWw83eJeo7"
      },
      "source": [
        "### üîç Build the Retrieval-Augmented Generation (RAG) Chain\n",
        "\n",
        "Now that we have our vectorstore populated with document chunks and embeddings, we can wire everything together into a Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "This RAG system uses a \"stuffing\" strategy, where all retrieved documents are concatenated into a single prompt before being passed to the language model.\n",
        "\n",
        "#### üîó Key Components:\n",
        "\n",
        "- **`vectorstore.as_retriever(k=8)`**  \n",
        "  Converts the FAISS vectorstore into a retriever object that finds the most semantically relevant document chunks for a user query.  \n",
        "  We use `k = 8` to retrieve the top 8 most relevant chunks.\n",
        "\n",
        "- **`create_stuff_documents_chain(...)`**  \n",
        "  Creates a LangChain chain that stuffs multiple documents into a prompt template and sends it to the LLM.  \n",
        "  This strategy is effective when the total input size remains within the LLM‚Äôs context window.\n",
        "\n",
        "- **`create_retrieval_chain(...)`**  \n",
        "  Wraps the retriever and the document chain into a full end-to-end pipeline:\n",
        "  1. A user query is passed to the retriever.\n",
        "  2. The retriever returns a list of relevant text chunks.\n",
        "  3. These chunks are inserted into a prompt template along with the original question.\n",
        "  4. The LLM generates an answer strictly based on the provided context.\n",
        "\n",
        "This architecture gives you a fully functional, local, open-source LLM-based assistant that can answer domain-specific questions using real documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReXQDoGENSxY"
      },
      "outputs": [],
      "source": [
        "# Define the LLM\n",
        "\n",
        "llm = OllamaLLM(\n",
        "    model=\"mistral\",\n",
        "    temperature=0.1,\n",
        "    stop=[\"<end_of_turn>\"],\n",
        ")\n",
        "\n",
        "# Build the LLM question-answering chain\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        ")\n",
        "\n",
        "# Configure the retriever\n",
        "\n",
        "NB_RETRIEVED_CHUNKS = 8\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": NB_RETRIEVED_CHUNKS}\n",
        ")\n",
        "\n",
        "# Combine the retriever + LLM chain into one Retrieval-Augmented Generation (RAG) pipeline\n",
        "\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain=question_answer_chain\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfvP8pB5JBQB"
      },
      "source": [
        "The temperature parameter in a language model (LLM) controls the randomness of the model's output.\n",
        "\n",
        "- A lower temperature value (closer to 0) makes the model more deterministic, favoring higher probability words and resulting in more predictable and repetitive text.\n",
        "\n",
        "- A higher temperature value (closer to 1) increases randomness, allowing for more creative and diverse responses by giving less probable words a better chance of being chosen.\n",
        "\n",
        "Adjusting the temperature helps balance between coherence and creativity in the generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh4YVPuYKyWJ"
      },
      "source": [
        "### Chatting with our RAG chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drPeKORpNWm3"
      },
      "outputs": [],
      "source": [
        "query = \"Where is this workshop?\"\n",
        "result = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "# ‚úÖ Print the generated answer\n",
        "console.print(Markdown(result[\"answer\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32u0WSbzmuUY"
      },
      "outputs": [],
      "source": [
        "# Retrieve the top relevant chunks\n",
        "retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "# Print the retrieved chunks\n",
        "print(f\"\\nüîç Top {NB_RETRIEVED_CHUNKS} Retrieved Chunks:\\n\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"--- Chunk #{i} ---\")\n",
        "    print(doc.page_content[:500].strip(), \"\\n\")  # print first 500 characters of each chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCy327uILLk4"
      },
      "source": [
        "Display the full result object (includes context docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSgCCh4yNhzk"
      },
      "outputs": [],
      "source": [
        "console.print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}